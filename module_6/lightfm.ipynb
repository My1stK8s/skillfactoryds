{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85Epq0XsHiV5"
   },
   "source": [
    "## Base model with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "EA6NYk-gHiV5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda33\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('recommendationsv4/train.csv')\n",
    "# delete duplicates\n",
    "train.drop_duplicates(inplace=True)\n",
    "# build user-item interactions matrix\n",
    "ratings_coo = sparse.coo_matrix(\n",
    "    (train['rating'].astype(int), (train['userid'], train['itemid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "rhqPNICEHiV5",
    "outputId": "0159820e-403a-4b09-f8b8-b275210197c7"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "NUM_THREADS = 2  # number of threads\n",
    "NUM_COMPONENTS = 20  # number of vector components\n",
    "NUM_EPOCHS = 10  # number of learning epochs\n",
    "\n",
    "# split ratings to train and test\n",
    "train_rating_coo, test_rating_coo = random_train_test_split(ratings_coo)\n",
    "\n",
    "# define model and train\n",
    "model = LightFM(learning_rate=0.1, loss='logistic',\n",
    "                no_components=NUM_COMPONENTS)\n",
    "model = model.fit(train_rating_coo, epochs=NUM_EPOCHS, num_threads=NUM_THREADS)\n",
    "\n",
    "# make predictions with test\n",
    "preds = model.predict(test_rating_coo.row, test_rating_coo.col)\n",
    "print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "LcCdogr6HiV6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# calculating metrics (ROC-AUC, precision_at_k, recall_at_k) and measuring execution time\\nstart_time = time.time()\\nprint(\\n    f\\'ROC-AUC (sklearn) = {sklearn.metrics.roc_auc_score(test_rating_coo.data,preds)}\\')\\nprint(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\\nstart_time = time.time()\\nprint(\\n    f\\'ROC-AUC (LFM) = {auc_score(model=model, test_interactions=test_rating_coo).mean()}\\')\\nprint(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\\nstart_time = time.time()\\nprint(\\n    f\\'Precision_at_k (LFM) = {precision_at_k(model=model, test_interactions=test_rating_coo, k=5).mean()}\\')\\nprint(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\\nstart_time = time.time()\\nprint(\\n    f\\'Recall_at_k (LFM) = {recall_at_k(model=model, test_interactions=test_rating_coo, k=5).mean()}\\')\\nprint(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# calculating metrics (ROC-AUC, precision_at_k, recall_at_k) and measuring execution time\n",
    "start_time = time.time()\n",
    "print(\n",
    "    f'ROC-AUC (sklearn) = {sklearn.metrics.roc_auc_score(test_rating_coo.data,preds)}')\n",
    "print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\n",
    "start_time = time.time()\n",
    "print(\n",
    "    f'ROC-AUC (LFM) = {auc_score(model=model, test_interactions=test_rating_coo).mean()}')\n",
    "print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\n",
    "start_time = time.time()\n",
    "print(\n",
    "    f'Precision_at_k (LFM) = {precision_at_k(model=model, test_interactions=test_rating_coo, k=5).mean()}')\n",
    "print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\n",
    "start_time = time.time()\n",
    "print(\n",
    "    f'Recall_at_k (LFM) = {recall_at_k(model=model, test_interactions=test_rating_coo, k=5).mean()}')\n",
    "print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAOfEv49HiV6"
   },
   "source": [
    "### Результаты \n",
    "\n",
    "* model fit-predict = (5 sec.)\n",
    "* ROC-AUC (sklearn) = 0.72814 (0.07 sec.)\n",
    "* ROC-AUC (LFM) = 0.71985 (192 sec.)\n",
    "* Precision_at_k (LFM) = 0.01306 (176 sec.)\n",
    "* Recall_at_k (LFM) = 0.04045 (196 sec.)\n",
    "\n",
    "Можно видеть, что метрики ROC-AUC практически одинаковы для обеих библиотек, однако время подсчёта отличается \n",
    "на порядки. В среднем, вычисление всех трех метрик встроенными функциями библиотеки LightFm происходит за время\n",
    "около трёх минут каждая, что почти в 40 раз превышает время обучения модели и предсказания. В дальнейшем при подборе гиперпараметров для экономии времени будем вычислять только \n",
    "ROC-AUC, а другие метрики - только лишь для финальной модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKYZtU65HiV7"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkHvnK9wHiV7"
   },
   "source": [
    "Попробуем теперь улучшить метрики за счёт подбора гиперпараметров. Используем самописный перебор по сетке параметров. \n",
    "\n",
    "Варьировать будем параметры: \n",
    "* loss_types = {\"logistic\", \"warp\"}\n",
    "* n_components = {20, 25, 30, 35}\n",
    "* n_epochs = {5, 10, 15, 20}\n",
    "\n",
    "Параметр, отвечающий за число кросс-валидаций выберем равным 3. Затем выберем оптимальную модель из условия наилучшего среднего значения усредненной метрики ROC-AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xKcQbHa0HiV7"
   },
   "outputs": [],
   "source": [
    "def solve(n_treads, n_components, n_epochs, loss_type, train_rating_coo, test_rating_coo):\n",
    "    \"\"\"function returns ROC_AUC score for current model\"\"\"\n",
    "    model = LightFM(learning_rate=0.1, loss=loss_type,\n",
    "                    no_components=n_components)\n",
    "    model = model.fit(train_rating_coo, epochs=n_epochs, num_threads=n_treads)\n",
    "    preds = model.predict(test_rating_coo.row, test_rating_coo.col)\n",
    "    return sklearn.metrics.roc_auc_score(test_rating_coo.data, preds)\n",
    "\n",
    "\n",
    "def solve_metrics(n_treads, n_components, n_epochs, loss_type, train_rating_coo, test_rating_coo):\n",
    "    \"\"\"function returns precision and recall at_k score for current model\"\"\"\n",
    "    model = LightFM(learning_rate=0.1, loss=loss_type,\n",
    "                    no_components=n_components)\n",
    "    model = model.fit(train_rating_coo, epochs=n_epochs, num_threads=n_treads)\n",
    "    start_time = time.time()\n",
    "    pr_at_k = precision_at_k(\n",
    "        model=model, test_interactions=test_rating_coo, k=5).mean()\n",
    "    print(f'Precision_at_k (LFM) = {pr_at_k}')\n",
    "    print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\n",
    "    start_time = time.time()\n",
    "    rc_at_k = recall_at_k(\n",
    "        model=model, test_interactions=test_rating_coo, k=5).mean()\n",
    "    print(f'Recall_at_k (LFM) = {rc_at_k}')\n",
    "    print(\"total time:\", round(time.time() - start_time, 2), \"sec.\")\n",
    "    return pr_at_k, rc_at_k\n",
    "\n",
    "\n",
    "def submission_best(loss_type, components, epochs_val, threads):\n",
    "    model = LightFM(learning_rate=0.1, loss=loss_type,\n",
    "                    no_components=components)\n",
    "    model = model.fit(ratings_coo, epochs=epochs_val, num_threads=threads)\n",
    "    preds = model.predict(test.userid.values, test.itemid.values)\n",
    "    preds = (preds - preds.min())/(preds.max() - preds.min())\n",
    "    submission['rating'] = preds\n",
    "    submission.to_csv('submission_base.csv', index=False)\n",
    "    print(\"Submission successifully wrote in file\")\n",
    "\n",
    "\n",
    "def write_model_embeddings(loss_type, components, epochs_val, threads):\n",
    "    \"\"\"function writes model and embeddings in files\"\"\"\n",
    "    model = LightFM(learning_rate=0.1, loss=loss_type,\n",
    "                    no_components=components)\n",
    "    model = model.fit(ratings_coo, epochs=epochs_val, num_threads=threads)\n",
    "    item_biases, item_embeddings = model.get_item_representations()\n",
    "    user_biases, user_embeddings = model.get_user_representations()\n",
    "    names = [\"item_embeddings\", \"user_embeddings\", \"model\"]\n",
    "    data = [item_embeddings, user_embeddings, model]\n",
    "    for idx, name in enumerate(names):\n",
    "        with open(name + '.pkl', 'wb') as file:\n",
    "            pickle.dump(data[idx], file)\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "5ueJfQU8HiWD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation best params:\n",
      "n_components: 74\n",
      "n_epochs: 14\n",
      "loss_type: logistic\n",
      "total time: 732.35 sec.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loss_types = [\"logistic\", \"warp\"]\n",
    "n_treads = 8\n",
    "n_components = [i for i in range(74, 75, 1)]\n",
    "n_epochs = [i for i in range(14, 15, 1)]\n",
    "n_cv = 3\n",
    "result = pd.DataFrame(\n",
    "    columns=['n_comp', 'n_epochs', 'loss_type', 'roc_auc', 'aver_roc_auc'])\n",
    "idx = 0\n",
    "all_iters = len(n_components) * len(n_epochs) * len(loss_types)\n",
    "start_time = time.time()\n",
    "for components in n_components:\n",
    "    for epochs in n_epochs:\n",
    "        for loss in loss_types:\n",
    "            roc_auc = []\n",
    "            models = []\n",
    "            for _ in range(n_cv):\n",
    "                train_rating_coo, test_rating_coo = random_train_test_split(\n",
    "                    ratings_coo)\n",
    "                answer = solve(n_treads, components, epochs, loss,\n",
    "                               train_rating_coo, test_rating_coo)\n",
    "                roc_auc.append(round(answer, 5))\n",
    "            aver_roc_auc = round(sum(roc_auc) / len(roc_auc), 4)\n",
    "            idx = idx + 1\n",
    "            new = pd.DataFrame(\n",
    "                [[components, epochs, loss, roc_auc, aver_roc_auc]], columns=result.columns)\n",
    "            result = pd.concat([result, new])\n",
    "            print(\n",
    "                f\"Progress: {idx}/{all_iters} ({round(100 * idx / all_iters,1)}%)\", end='\\r')\n",
    "end_time = time.time()\n",
    "result.index = [i + 1 for i in range(idx)]\n",
    "result.sort_values(by=['aver_roc_auc'], ascending=False, inplace=True)\n",
    "result.to_csv('result_cv.csv')\n",
    "best = result.iloc[0].values\n",
    "best_comp = best[0]\n",
    "best_epochs = best[1]\n",
    "best_loss = best[2]\n",
    "print(\"Cross-validation best params:\")\n",
    "print(f\"n_components: {best_comp}\")\n",
    "print(f\"n_epochs: {best_epochs}\")\n",
    "print(f\"loss_type: {best_loss}\")\n",
    "print(\"total time:\", round(end_time - start_time, 2), \"sec.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqTUi9kpHiWI"
   },
   "source": [
    "Из параметров перебора оптимальными стали:\n",
    "* loss_types = \"logistic\"\n",
    "* n_components = 74\n",
    "* n_epochs = 14\n",
    "\n",
    "Усредненое значение метрики ROC-AUC = 0.7291, что является улучшением результата, полученного с базовыми параметрами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZDfwTZIHiWI"
   },
   "source": [
    "Подсчитаем среднее значение метрик precision and recall at_k для полученных параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQbHIFIrHiWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_at_k (LFM) = 0.016070792451500893\n",
      "total time: 761.89 sec.\n",
      "Recall_at_k (LFM) = 0.04852670181704106\n",
      "total time: 760.52 sec.\n",
      "Precision_at_k (LFM) = 0.0159852784126997\n",
      "total time: 762.8 sec.\n"
     ]
    }
   ],
   "source": [
    "prec_at_k = []\n",
    "rec_at_k = []\n",
    "for _ in range(n_cv):\n",
    "    train_rating_coo, test_rating_coo = random_train_test_split(ratings_coo)\n",
    "    answer = solve_metrics(n_treads, best_comp, best_epochs,\n",
    "                           best_loss, train_rating_coo, test_rating_coo)\n",
    "    prec_at_k.append(round(answer[0], 5))\n",
    "    rec_at_k.append(round(answer[1], 5))\n",
    "aver_pr_at_k = round(sum(prec_at_k) / len(prec_at_k), 4)\n",
    "aver_rc_at_k = round(sum(rec_at_k) / len(rec_at_k), 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0atmmejgHiWJ"
   },
   "source": [
    "### Результаты\n",
    "\n",
    "* Precision_at_k (LFM) = 0.0165\n",
    "* Recall_at_k (LFM) = 0.0500\n",
    "\n",
    "Можно видеть, что значение метрик precision и recall at_k также улучшились по сравнению с базовыми параметрами "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzOuq1jRHiWJ"
   },
   "source": [
    "### Выводы\n",
    "\n",
    "построена базовая модель, основанная на матрице взаимодействий user-item. Проведен подбор гиперпараметров,который показал улучшение значений метрик ROC-AUC, precision_at_k, recall_at_k. Значение метрики ROC-AUC, полученное в соревновании kaggle равно 0.76859. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make submission with optimal parametres\n",
    "submission_best(best_loss, best_comp, best_epochs, n_treads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00FdAjF-HiWK"
   },
   "source": [
    "## Prototype prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czxKuIr5HiWK"
   },
   "source": [
    "\n",
    "Запишем файлы модели и представления user и item в файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUgVoRdCHiWL"
   },
   "outputs": [],
   "source": [
    "write_model_embeddings(\"logistic\", 75, 15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2oIWNFUHiWL"
   },
   "source": [
    "Загрузим данные о товарах:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MZmPZcCHiWM"
   },
   "source": [
    "Сформируем датафрейм items, куда для каждого item запишем общее число отзывов (Reviews), asin (Amazon Standard Identification Number) и среднюю оценку (aver_mark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = []\n",
    "with open('recommendationsv4/meta_Grocery_and_Gourmet_Food.json', 'r') as file:\n",
    "    for line in file:\n",
    "        meta.append(json.loads(line))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaTeeq_iHiWM"
   },
   "outputs": [],
   "source": [
    "items = train[['itemid', 'unixReviewTime']].groupby('itemid').count()\n",
    "items['asin'] = train[['itemid', 'asin']].groupby('itemid').first()\n",
    "items.columns = ['Reviews', 'asin']\n",
    "items['aver_mark'] = train[['itemid', 'overall']].groupby('itemid').mean()\n",
    "items['item_id'] = items.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZJx0_FWHiWN"
   },
   "source": [
    "Добавим также имя для каждого item, соединив items и meta по полю asin и сохраним в файл: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyS04ZMaHiWO"
   },
   "outputs": [],
   "source": [
    "meta = pd.DataFrame(meta)[[\"title\", \"asin\"]]\n",
    "meta.drop_duplicates(subset=['asin'], keep='last', inplace=True)\n",
    "items = items.merge(meta, how='left')\n",
    "items.to_csv('items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89x18QIVHiWO"
   },
   "source": [
    "Построим и сохраним в файлы графы для быстрого поиска "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCg92ZpeHiWO"
   },
   "outputs": [],
   "source": [
    "names = [\"item_embeddings\", \"user_embeddings\"]\n",
    "out_names = [\"item_graph\", \"user_graph\"]\n",
    "for idx, name in enumerate(names):\n",
    "    with open(name + '.pkl', 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "        file.close()\n",
    "    search_graph_item = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    search_graph_item.addDataPointBatch(embeddings)\n",
    "    search_graph_item.createIndex()\n",
    "    search_graph_item.saveIndex(out_names[idx] + \".hnsw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z4G8tLVHiWP"
   },
   "source": [
    "Напишем пробный вариант поиска рекомендаций для item и user, из которого затем создадим прототип в heroku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gj-f8s0GHiWP",
    "outputId": "94ba2767-eb3f-45d2-f07c-6d77160284d3"
   },
   "outputs": [],
   "source": [
    "def get_titles(items_list, items):\n",
    "    result = pd.DataFrame(items[items.item_id.isin(items_list)].title)\n",
    "    result.index = [i + 1 for i in range(len(items_list))]\n",
    "    return result\n",
    "\n",
    "\n",
    "def nearest(id, embeddings, graph, n=5):\n",
    "    # n+1 because id includes in nearest list too\n",
    "    return graph.knnQuery(embeddings[id], k=n+1)[0][1:]\n",
    "\n",
    "\n",
    "def load_embeddings(name):\n",
    "    # load embeddings\n",
    "    with open(name, 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "        file.close()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_items():\n",
    "    # load items\n",
    "    return pd.read_csv('items.csv')\n",
    "\n",
    "\n",
    "item_embeddings = load_embeddings('item_embeddings.pkl')\n",
    "items = load_items()\n",
    "search_graph_item = nmslib.init()\n",
    "search_graph_item.loadIndex(\"item_graph.hnsw\")\n",
    "\n",
    "n_recommends = 10\n",
    "\n",
    "# find nearest items:\n",
    "item_values = list(items.item_id)\n",
    "input_str = \"100\"\n",
    "item_for_search = int(input_str)\n",
    "if item_for_search in item_values:\n",
    "    title = get_titles([item_for_search], items)\n",
    "    print(\"Item title:\")\n",
    "    print(title.title)\n",
    "    nearest_item = nearest(int(item_for_search),\n",
    "                           item_embeddings, search_graph_item, n_recommends)\n",
    "    answer = get_titles(nearest_item, items)\n",
    "    print(\"Recommendations:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Sorry, item_id =\" + \" {\" + input_str + \"} \" + \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2E_uKGyHiWP",
    "outputId": "ff9df80e-7661-413f-c1ea-56385e53f29b"
   },
   "outputs": [],
   "source": [
    "def find_best_items(user_id):\n",
    "    best_items = np.array([])\n",
    "    if user_id in exist_users:\n",
    "        # only good marks (4, 5)\n",
    "        temp = train[(train['userid'] == user_id) & (\n",
    "            train['overall'].isin([5.0, 4.0]))]\n",
    "        best_items = temp.sort_values(by=['overall'], ascending=False)[\n",
    "            'itemid'].values[:best_n]\n",
    "        # if we need more (did not find all of <best_n> items for current user)\n",
    "        if len(best_items) < best_n:\n",
    "            add = best_n - len(best_items)\n",
    "            # find nearest users for current\n",
    "            near_users = nearest(user_id, user_embeddings, search_graph_user)\n",
    "            for user in near_users:\n",
    "                temp = train[(train['userid'] == user) & (\n",
    "                    train['overall'].isin([5.0, 4.0]))]\n",
    "                temp_best = temp.sort_values(by=['overall'], ascending=False)[\n",
    "                    'itemid'].values[:add]\n",
    "                best_items = np.concatenate((best_items, temp_best))\n",
    "                add = best_n - len(best_items)\n",
    "                if len(best_items) == best_n:\n",
    "                    return best_items\n",
    "    return best_items\n",
    "\n",
    "\n",
    "def find_recommend_items(best_items):\n",
    "    recommend_items = set()\n",
    "    list_items = []\n",
    "    # if found any best items for user, build a recommend list\n",
    "    if (len(best_items) <= best_n) and (len(best_items) > 0):\n",
    "        for item in best_items:\n",
    "            recommend_items.update(\n",
    "                nearest(item, item_embeddings, search_graph_item))\n",
    "            list_items = list(recommend_items)\n",
    "            if len(list_items) >= n_recommends:\n",
    "                list_items = list_items[0:n_recommends]\n",
    "                break\n",
    "    # if we need more recommendations, take items with good aver\n",
    "    # marks and lot of reviews (popular)\n",
    "    if len(list_items) != n_recommends:\n",
    "        most_popular_items = items[(items.aver_mark > 4.5)\n",
    "                                   & (items.Reviews > items['Reviews'].quantile(0.99))]\n",
    "    # add random recommendation in recommendation list from popular\n",
    "    while len(list_items) < n_recommends:\n",
    "        pop_id = most_popular_items.sample(10).item_id.values\n",
    "        for idx in pop_id:\n",
    "            if idx not in list_items:\n",
    "                list_items.append(idx)\n",
    "\n",
    "    return list_items[0:n_recommends]\n",
    "\n",
    "\n",
    "def find_popular_items():\n",
    "    list_items = []\n",
    "    most_popular_items = items[(items.aver_mark > 4.5)\n",
    "                               & (items.Reviews > items['Reviews'].quantile(0.99))]\n",
    "    # add random recommendation in recommendation list from popular\n",
    "    while len(list_items) < n_recommends:\n",
    "        pop_id = most_popular_items.sample(n_recommends).item_id.values\n",
    "        for idx in pop_id:\n",
    "            if idx not in list_items:\n",
    "                list_items.append(idx)\n",
    "    return list_items[0:n_recommends]\n",
    "\n",
    "\n",
    "best_n = 3\n",
    "item_embeddings = load_embeddings('user_embeddings.pkl')\n",
    "items = load_items()\n",
    "search_graph_user = nmslib.init()\n",
    "search_graph_user.loadIndex(\"user_graph.hnsw\")\n",
    "\n",
    "exist_users = train['userid'].unique()\n",
    "input_str = \"101\"\n",
    "user_for_recommend = int(input_str)\n",
    "\n",
    "if user_for_recommend in exist_users:\n",
    "    best_items = find_best_items(user_for_recommend)\n",
    "    list_items = find_recommend_items(best_items)\n",
    "else:\n",
    "    list_items = find_popular_items()\n",
    "answer = get_titles(list_items, items)\n",
    "print(\"Recommendations: \")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPBsbf5sHiWQ"
   },
   "source": [
    "### Выводы\n",
    "\n",
    "написан код, дающий рекомендации как к item_id, так и к user_id. Построен прототип с использованием сервиса heroku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
